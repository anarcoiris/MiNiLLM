================================================================================
MINI-LLM v2 - PARCHES CRÍTICOS
================================================================================
Fecha: 2025-01-XX
Versión: v2.1

Este archivo contiene todos los cambios necesarios para corregir los bugs
críticos identificados en el análisis de código.

RESUMEN:
- 7 parches en total
- 3 críticos (bugs que afectan funcionalidad)
- 2 medios (mejoras de rendimiento/estabilidad)
- 2 bajos (limpieza de código y documentación)

================================================================================
PATCH 1/7: Validación de stride en dataset.py
================================================================================
SEVERIDAD: CRÍTICA
ARCHIVO: dataset.py
LÍNEA: ~66
DESCRIPCIÓN: Añade validación para evitar stride > block_size que causaría
             pérdida de cobertura en el corpus.

--- dataset.py (original)
+++ dataset.py (corregido)
@@ -66,6 +66,14 @@
         # Stride por defecto: 50% overlap
         self.stride = stride if stride is not None else block_size // 2
+        
+        # Validación: stride no puede ser mayor que block_size
+        if self.stride > self.block_size:
+            raise ValueError(
+                f"stride ({self.stride}) no puede ser mayor que block_size ({self.block_size}). "
+                f"Esto causaría saltos en la cobertura del corpus."
+            )
+        
+        if self.stride <= 0:
+            raise ValueError(f"stride debe ser > 0, recibido: {self.stride}")
 
         # Longitud mínima por defecto: al menos 2 tokens más que block_size
         self.min_doc_length = min_doc_length if min_doc_length is not None else block_size + 2

IMPACTO:
- Previene configuraciones inválidas que causarían pérdida de datos
- Mejora mensajes de error para usuarios

TEST:
```python
# Debería fallar
dataset = DocumentAwareDataset(ids, block_size=128, stride=200)
# ValueError: stride (200) no puede ser mayor que block_size (128)
```

================================================================================
PATCH 2/7: Corrección de lógica last_start (modo normal)
================================================================================
SEVERIDAD: CRÍTICA
ARCHIVO: dataset.py
LÍNEA: ~136-141
DESCRIPCIÓN: Corrige la lógica de añadir la última ventana en documentos para
             asegurar cobertura completa hasta el final.

--- dataset.py (original)
+++ dataset.py (corregido)
@@ -136,10 +136,13 @@
                 # Añade última ventana si no está ya incluida
-                last_start = doc_len - block_size - 1
-                if last_start >= 0 and (not self.samples or self.samples[-1][1] < last_start):
-                    self.samples.append((doc_idx, last_start))
-                    for pos in range(last_start, doc_len):
-                        unique_tokens_covered.add((doc_idx, pos))
+                # Asegura que cubrimos hasta el final del documento
+                last_possible_start = doc_len - block_size - 1
+                if last_possible_start >= 0:
+                    # Verifica si la última sample cubre el final del documento
+                    if not self.samples or (self.samples[-1][1] + block_size < doc_len):
+                        # Necesitamos otra ventana para cubrir el final
+                        self.samples.append((doc_idx, last_possible_start))
+                        for pos in range(last_possible_start, min(last_possible_start + block_size + 1, doc_len)):
+                            unique_tokens_covered.add((doc_idx, pos))

PROBLEMA CORREGIDO:
Antes: Con doc_len=300, block_size=128, stride=64
       Samples: 0, 64, 128, 192
       last_start = 171
       Condición: 192 < 171 = False → NO añade sample
       Resultado: Tokens 256-299 sin cubrir

Ahora: Verifica correctamente si necesita última ventana
       Si samples[-1][1] + block_size < doc_len → añade ventana
       Asegura cobertura completa

================================================================================
PATCH 3/7: Corrección de lógica last_start (modo memory-efficient)
================================================================================
SEVERIDAD: CRÍTICA
ARCHIVO: dataset.py
LÍNEA: ~156-161
DESCRIPCIÓN: Misma corrección que PATCH 2 pero para el modo memory-efficient.

--- dataset.py (original)
+++ dataset.py (corregido)
@@ -156,10 +156,13 @@
-                last_start = doc_len - block_size - 1
-                if last_start >= 0 and (not self.samples or self.samples[-1][1] < last_start):
-                    self.samples.append((doc_idx, last_start))
-                    for pos in range(last_start, doc_len):
-                        unique_tokens_covered.add((doc_idx, pos))
+                last_possible_start = doc_len - block_size - 1
+                if last_possible_start >= 0:
+                    if not self.samples or (self.samples[-1][1] + block_size < doc_len):
+                        self.samples.append((doc_idx, last_possible_start))
+                        for pos in range(last_possible_start, min(last_possible_start + block_size + 1, doc_len)):
+                            unique_tokens_covered.add((doc_idx, pos))

IMPACTO:
- Asegura cobertura completa en modo memory-efficient
- Previene pérdida de tokens al final de documentos

================================================================================
PATCH 4/7: Limpieza de GPU memory entre epochs
================================================================================
SEVERIDAD: MEDIA
ARCHIVO: training.py
LÍNEA: ~290-295
DESCRIPCIÓN: Añade torch.cuda.empty_cache() entre epochs para liberar memoria
             fragmentada y prevenir acumulación de memoria GPU.

--- training.py (original)
+++ training.py (corregido)
@@ -290,6 +290,10 @@
         # End of epoch evaluation
         epoch_train_loss = epoch_loss / epoch_tokens
         val_loss = evaluate(model, val_loader, device)
+        
+        # Limpia caché de GPU para liberar memoria fragmentada
+        if torch.cuda.is_available():
+            torch.cuda.empty_cache()
 
         print(f"\n{'='*70}")
         print(f"Epoch {epoch + 1} Summary:")

IMPACTO:
- Reduce fragmentación de memoria GPU en entrenamientos largos
- Puede prevenir OOM en epochs tardíos
- Mínimo impacto en performance (~0.1s por epoch)

================================================================================
PATCH 5/7: Validación de vocab_size extremos
================================================================================
SEVERIDAD: BAJA
ARCHIVO: main.py
LÍNEA: ~491-495
DESCRIPCIÓN: Añade warnings para vocabularios muy pequeños o muy grandes.

--- main.py (original)
+++ main.py (corregido)
@@ -491,6 +491,14 @@
     tok = validate_tokenizer(tokenizer_path)
     tw = HFTokenizerWrapper(tok)
     console.print(f"   Vocab size: {tw.vocab_size:,}")
+    
+    # Validación de vocab_size
+    if tw.vocab_size < 1000:
+        console.print("[yellow]⚠️  Advertencia: Vocabulario muy pequeño (<1000). "
+                     "Esto puede causar underfitting y mala representación del texto.[/]")
+    elif tw.vocab_size > 100000:
+        console.print("[yellow]⚠️  Advertencia: Vocabulario muy grande (>100K). "
+                     "Esto aumentará significativamente el uso de memoria.[/]")

IMPACTO:
- Alerta temprana sobre configuraciones subóptimas
- Ayuda a usuarios a entender trade-offs

================================================================================
PATCH 6/7: Eliminar variable sin uso
================================================================================
SEVERIDAD: BAJA
ARCHIVO: main.py
LÍNEA: ~474-476
DESCRIPCIÓN: Elimina variable mock_ids que nunca se utiliza.

--- main.py (original)
+++ main.py (corregido)
@@ -471,9 +471,6 @@
     # Lee corpus
     text = validate_corpus(corpus_path)
 
-    # Mock tokenizer simple (split por espacios)
-    mock_ids = list(range(len(text.split())))
-
     console.print(f"   Chars: {len(text):,}")
     console.print(f"   Palabras (aprox): {len(text.split()):,}")

IMPACTO:
- Limpieza de código
- Sin impacto funcional

================================================================================
PATCH 7/7: Documentar impacto de memory_efficient
================================================================================
SEVERIDAD: BAJA
ARCHIVO: dataset.py
LÍNEA: ~53
DESCRIPCIÓN: Mejora documentación sobre el impacto de performance del modo
             memory_efficient.

--- dataset.py (original)
+++ dataset.py (corregido)
@@ -53 +53,4 @@
-        memory_efficient: Si True, no guarda documentos en memoria (más lento pero menos RAM)
+        memory_efficient: Si True, no guarda documentos en memoria (más lento pero menos RAM).
+                          ADVERTENCIA: Puede reducir velocidad de training en 20-30% debido a slicing.
+                          Úsalo solo si el corpus no cabe en RAM (>4GB de tokens).

IMPACTO:
- Mejor información para tomar decisiones
- Usuarios sabrán cuándo usar esta opción

================================================================================
INSTRUCCIONES DE APLICACIÓN
================================================================================

OPCIÓN 1: Usar script automático (RECOMENDADO)
-----------------------------------------------
1. Guarda patches.py en el directorio del proyecto
2. Verifica qué se aplicará:
   python patches.py --check

3. Aplica los parches (crea backups automáticamente):
   python patches.py --apply

4. Si algo sale mal, restaura:
   python patches.py --restore


OPCIÓN 2: Aplicar manualmente
------------------------------
1. Crea backups de los archivos:
   cp dataset.py dataset.py.backup
   cp training.py training.py.backup
   cp main.py main.py.backup

2. Abre cada archivo y busca las secciones indicadas (usa las líneas aprox.)

3. Reemplaza el código "original" por el código "corregido"

4. Ejecuta tests para verificar:
   python dataset.py
   python training.py


OPCIÓN 3: Usar git (si usas control de versiones)
--------------------------------------------------
1. Guarda este archivo como mini-llm-v2-fixes.patch

2. Aplica con git apply:
   git apply mini-llm-v2-fixes.patch

3. Si hay conflictos, resuelve manualmente


================================================================================
VALIDACIÓN POST-PARCHE
================================================================================

Después de aplicar los parches, ejecuta:

1. Tests unitarios:
   python dataset.py      # Debe mostrar: ✅ TODOS LOS TESTS PASADOS
   python generation.py   # Debe mostrar: ✅ TODOS LOS TESTS PASADOS
   python model.py        # Debe mostrar: ✅ TODOS LOS TESTS PASADOS
   python training.py     # Debe mostrar: ✅ All tests passed!

2. Test de validación de stride:
   python -c "
from dataset import DocumentAwareDataset
try:
    ds = DocumentAwareDataset([1,2,3]*100, block_size=10, stride=20)
    print('❌ ERROR: Debería haber fallado')
except ValueError as e:
    print(f'✅ Validación funcionando: {e}')
"